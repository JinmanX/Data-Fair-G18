{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ekphrasis.classes.segmenter import Segmenter\n",
    "from wordsegment import load, segment\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter.csv')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df =df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df[df.index.month == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df9.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(s):\n",
    "    s = re.findall(r\"#(\\w+)\", s)\n",
    "    return ' '.join(s)\n",
    "df9['all_hashtags'] = df9['tweets'].apply(lambda x: extract_hashtags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_hashtags(s):\n",
    "    s = ' '.join(segment(s))\n",
    "    #hashtag_tokens=word_tokenize(string)\n",
    "    #return \" \".join(hashtag_tokens)\n",
    "    return s\n",
    "df9['all_hashtags_seg'] = df9['all_hashtags'].apply(lambda x: segment_hashtags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~“”…’'\n",
    "\n",
    "def reduce_noise(text):\n",
    "    ##mentions, hashtags, urls and emojis\n",
    "    text = re.sub(r\"#(\\w+)\",'', text)\n",
    "    text = re.sub(r\"@\\w+\",'', text)\n",
    "    text = re.sub(r'\\\\x..','',text)\n",
    "    text = re.sub(\"\\n\",'',text)\n",
    "    text = re.sub(r\"((http|https|ftp|file|www)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|])\",'', text)\n",
    "    ##html entities\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r'[0-9]+', '', text)#'\\d+'\n",
    "    text = \"\".join([i for i in text if i not in punc])\n",
    "    return text\n",
    "df9['tweets_clean'] = df9['tweets'].apply(lambda x: reduce_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in sw]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['tweets_token'] = df9['tweets_clean'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df9['tweets_nonstop'] = df9['tweets_token'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df9['tags_token'] = df9['all_hashtags_seg'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df9['tags_nonstop'] = df9['tags_token'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corpus = [item for sublist in df9['tags_nonstop'] for item in sublist]\n",
    "counter = Counter(corpus)\n",
    "\n",
    "df_count = pd.DataFrame(counter.most_common(30))\n",
    "df_count.columns = ['Common_words','count']\n",
    "df_count.style.background_gradient(cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_aze_label = ['azerbaijan','armenia','armenian','azerbaijani','karabakh','nagorno','artsakh','aliyev']\n",
    "usa_label = ['trump','obama','usa','harris','biden']\n",
    "turkey_label = ['erdogan','turkey','istanbul']\n",
    "covid_label = ['covid','covid19','corona','virus','pandemic','coronavirus']\n",
    "yemen_label = ['yemen','yemeni']\n",
    "africa_label = ['africa','ethiopia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(text): \n",
    "    if any(w in text for w in arm_aze_label):\n",
    "        return 'Azerbaijan_Armenia'\n",
    "    elif any(w in text for w in turkey_label): \n",
    "        return 'Turkey'\n",
    "    elif any(w in text for w in usa_label): \n",
    "        return 'USA'\n",
    "    elif any(w in text for w in covid_label): \n",
    "        return 'covid'\n",
    "    elif any(w in text for w in yemen_label): \n",
    "        return 'Yemen'\n",
    "    elif any(w in text for w in africa_label): \n",
    "        return 'Africa'\n",
    "    else: \n",
    "        return 'Other'\n",
    "    \n",
    "df9['label'] = df9['all_hashtags_seg'].apply(lambda x: get_labels(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df9[~df9['label'].isin(['Other'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps = nltk.PorterStemmer()\n",
    "#def stemming(text):\n",
    "    #text = [ps.stem(word) for word in text]\n",
    "    #return text\n",
    "\n",
    "#wl = nltk.WordNetLemmatizer()\n",
    "#def lemmatizer(text):\n",
    "    #text = [wl.lemmatize(word) for word in text]\n",
    "    #return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['tweets_stem'] = df['tweets_nonstop'].apply(lambda x: stemming(x))\n",
    "#df['tags_stem'] = df['tags_nonstop'].apply(lambda x: stemming(x))\n",
    "#df['tweets_lem'] = df['tweets_stem'].apply(lambda x: lemmatizer(x))\n",
    "#df['tags_lem'] = df['tags_stem'].apply(lambda x: lemmatizer(x))\n",
    "#df['tweets_pos'] = df['tweets_stem'].apply(lambda x: nltk.pos_tag(x))\n",
    "#df['tags_pos'] = df['tags_stem'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from geopy.geocoders import Nominatim\n",
    "#from geopy.extra.rate_limiter import RateLimiter\n",
    "#locator = Nominatim(user_agent='myGeocoder')\n",
    "#geocode = RateLimiter(locator.geocode, min_delay_seconds=1)\n",
    "#df['geo_location'] = df['location'].apply(geocode)\n",
    "#df['point'] = df['geo_location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "#df[['latitude', 'longitude','altitude']] = pd.DataFrame(df['point'].tolist(), index=clean_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
