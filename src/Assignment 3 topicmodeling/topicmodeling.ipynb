{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk \n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bTwitter11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df =df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in sw]\n",
    "    return text\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wl = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wl.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags_token'] = df['all_hashtags_seg'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df['tags_nonstop'] = df['tags_token'].apply(lambda x: remove_stopwords(x))\n",
    "df['tags_stem'] = df['tags_nonstop'].apply(lambda x: stemming(x))\n",
    "df['tags_lem'] = df['tags_stem'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagstr(s):\n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] =  df['tags_nonstop'].apply(lambda x: tagstr(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>friends</th>\n",
       "      <th>followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>all_hashtags</th>\n",
       "      <th>all_hashtags_seg</th>\n",
       "      <th>tags_token</th>\n",
       "      <th>tags_nonstop</th>\n",
       "      <th>tags_stem</th>\n",
       "      <th>tags_lem</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-01 23:59:19</td>\n",
       "      <td>peace</td>\n",
       "      <td>To all whom I loved in this #life and to all t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>metsbookworm973</td>\n",
       "      <td>Inverness, FL</td>\n",
       "      <td>600</td>\n",
       "      <td>520</td>\n",
       "      <td>0</td>\n",
       "      <td>life AllSaintsDay pray peace God Father Heaven</td>\n",
       "      <td>life all saints day pray peace godfather heaven</td>\n",
       "      <td>[life, all, saints, day, pray, peace, godfathe...</td>\n",
       "      <td>[life, saints, day, pray, peace, godfather, he...</td>\n",
       "      <td>[life, saint, day, pray, peac, godfath, heaven]</td>\n",
       "      <td>[life, saint, day, pray, peac, godfath, heaven]</td>\n",
       "      <td>life saints day pray peace godfather heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-01 23:51:15</td>\n",
       "      <td>peace</td>\n",
       "      <td>You can't be strong all the time. Sometimes yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zzaqwan02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>peace</td>\n",
       "      <td>peace</td>\n",
       "      <td>[peace]</td>\n",
       "      <td>[peace]</td>\n",
       "      <td>[peac]</td>\n",
       "      <td>[peac]</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-01 23:46:27</td>\n",
       "      <td>peace</td>\n",
       "      <td>Meu teto é o céu!!\\n#chapadadosveadeiros #sky ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>valtissimos</td>\n",
       "      <td>Goiânia - Goiás - Brasil</td>\n",
       "      <td>290</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>chapadadosveadeiros sky sol peace vibes goodvibes</td>\n",
       "      <td>chapada dosveadeirosskysol peace vibes good vibes</td>\n",
       "      <td>[chapada, dosveadeirosskysol, peace, vibes, go...</td>\n",
       "      <td>[chapada, dosveadeirosskysol, peace, vibes, go...</td>\n",
       "      <td>[chapada, dosveadeirosskysol, peac, vibe, good...</td>\n",
       "      <td>[chapada, dosveadeirosskysol, peac, vibe, good...</td>\n",
       "      <td>chapada dosveadeirosskysol peace vibes good vibes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-01 23:41:04</td>\n",
       "      <td>peace</td>\n",
       "      <td>#cubans #Latinos #Hispanics #blackvoter this #...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>plzhelpkids</td>\n",
       "      <td>Anywhere, usa</td>\n",
       "      <td>900</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>cubans Latinos Hispanics blackvoter ElectionDa...</td>\n",
       "      <td>cubans latinos hispanics black voter election ...</td>\n",
       "      <td>[cubans, latinos, hispanics, black, voter, ele...</td>\n",
       "      <td>[cubans, latinos, hispanics, black, voter, ele...</td>\n",
       "      <td>[cuban, latino, hispan, black, voter, elect, d...</td>\n",
       "      <td>[cuban, latino, hispan, black, voter, elect, d...</td>\n",
       "      <td>cubans latinos hispanics black voter election ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-01 23:38:00</td>\n",
       "      <td>peace</td>\n",
       "      <td>Teach #peace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>RODRIGObessa</td>\n",
       "      <td>Belo Horizonte, Brasil</td>\n",
       "      <td>4,557</td>\n",
       "      <td>3,926</td>\n",
       "      <td>0</td>\n",
       "      <td>peace</td>\n",
       "      <td>peace</td>\n",
       "      <td>[peace]</td>\n",
       "      <td>[peace]</td>\n",
       "      <td>[peac]</td>\n",
       "      <td>[peac]</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp hashtags  \\\n",
       "0  2020-11-01 23:59:19    peace   \n",
       "1  2020-11-01 23:51:15    peace   \n",
       "2  2020-11-01 23:46:27    peace   \n",
       "3  2020-11-01 23:41:04    peace   \n",
       "4  2020-11-01 23:38:00    peace   \n",
       "\n",
       "                                              tweets  replies retweets likes  \\\n",
       "0  To all whom I loved in this #life and to all t...      NaN      NaN   NaN   \n",
       "1  You can't be strong all the time. Sometimes yo...      NaN      NaN   NaN   \n",
       "2  Meu teto é o céu!!\\n#chapadadosveadeiros #sky ...      NaN      NaN     1   \n",
       "3  #cubans #Latinos #Hispanics #blackvoter this #...      3.0        1     1   \n",
       "4                                       Teach #peace      NaN      NaN     1   \n",
       "\n",
       "          username                  location friends followers  verified  \\\n",
       "0  metsbookworm973             Inverness, FL     600       520         0   \n",
       "1        Zzaqwan02                       NaN       4         3         0   \n",
       "2      valtissimos  Goiânia - Goiás - Brasil     290       104         0   \n",
       "3      plzhelpkids             Anywhere, usa     900       157         0   \n",
       "4     RODRIGObessa    Belo Horizonte, Brasil   4,557     3,926         0   \n",
       "\n",
       "                                        all_hashtags  \\\n",
       "0     life AllSaintsDay pray peace God Father Heaven   \n",
       "1                                              peace   \n",
       "2  chapadadosveadeiros sky sol peace vibes goodvibes   \n",
       "3  cubans Latinos Hispanics blackvoter ElectionDa...   \n",
       "4                                              peace   \n",
       "\n",
       "                                    all_hashtags_seg  \\\n",
       "0    life all saints day pray peace godfather heaven   \n",
       "1                                              peace   \n",
       "2  chapada dosveadeirosskysol peace vibes good vibes   \n",
       "3  cubans latinos hispanics black voter election ...   \n",
       "4                                              peace   \n",
       "\n",
       "                                          tags_token  \\\n",
       "0  [life, all, saints, day, pray, peace, godfathe...   \n",
       "1                                            [peace]   \n",
       "2  [chapada, dosveadeirosskysol, peace, vibes, go...   \n",
       "3  [cubans, latinos, hispanics, black, voter, ele...   \n",
       "4                                            [peace]   \n",
       "\n",
       "                                        tags_nonstop  \\\n",
       "0  [life, saints, day, pray, peace, godfather, he...   \n",
       "1                                            [peace]   \n",
       "2  [chapada, dosveadeirosskysol, peace, vibes, go...   \n",
       "3  [cubans, latinos, hispanics, black, voter, ele...   \n",
       "4                                            [peace]   \n",
       "\n",
       "                                           tags_stem  \\\n",
       "0    [life, saint, day, pray, peac, godfath, heaven]   \n",
       "1                                             [peac]   \n",
       "2  [chapada, dosveadeirosskysol, peac, vibe, good...   \n",
       "3  [cuban, latino, hispan, black, voter, elect, d...   \n",
       "4                                             [peac]   \n",
       "\n",
       "                                            tags_lem  \\\n",
       "0    [life, saint, day, pray, peac, godfath, heaven]   \n",
       "1                                             [peac]   \n",
       "2  [chapada, dosveadeirosskysol, peac, vibe, good...   \n",
       "3  [cuban, latino, hispan, black, voter, elect, d...   \n",
       "4                                             [peac]   \n",
       "\n",
       "                                                tags  \n",
       "0        life saints day pray peace godfather heaven  \n",
       "1                                              peace  \n",
       "2  chapada dosveadeirosskysol peace vibes good vibes  \n",
       "3  cubans latinos hispanics black voter election ...  \n",
       "4                                              peace  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer()\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(df['tags'])#).toarray()\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=45)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "number_of_topics = 10\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n",
    "# Fit data to model\n",
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: peace zen yoga war quotes buddha art wisdom love mindfulness\n",
      "Topic #1: conflict war history politics climate china truth world justice military\n",
      "Topic #2: azerbaijan armenia war karabakh nagorno conflict artsakh peace armenian russia\n",
      "Topic #3: peace love god jesus joy conflict positive hope faith bible\n",
      "Topic #4: war stop artsakh turkey peace day erdogan conflict remembrance terrorist\n",
      "Topic #5: peace conflict war team love rights management human world psychic\n",
      "Topic #6: conflict peace vote united congress war states america day freedom\n",
      "Topic #7: peace war conflict music love life film protest action writing\n",
      "Topic #8: peace election war love 2020 trump biden humanity life vote\n",
      "Topic #9: peace love nature spiritual music life self meditation unity health\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([tf_feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love peace</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop erdogan</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>war artsakh</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>artsakh republic</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peace love</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>turkey stop</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>terrorist turkey</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>republic terrorist</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>defend artsakh</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>erdogan defend</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>remembrance day</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nagorno karabakh</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of the</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>conflict conflict</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>we forget</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>peace for</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>war crimes</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lest we</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>war peace</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the day</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bigrams  count\n",
       "0           love peace    109\n",
       "1         stop erdogan     93\n",
       "2          war artsakh     81\n",
       "3     artsakh republic     78\n",
       "4           peace love     76\n",
       "5          turkey stop     72\n",
       "6     terrorist turkey     69\n",
       "7   republic terrorist     68\n",
       "8       defend artsakh     62\n",
       "9       erdogan defend     61\n",
       "10     remembrance day     52\n",
       "11    nagorno karabakh     49\n",
       "12              of the     40\n",
       "13   conflict conflict     39\n",
       "14           we forget     37\n",
       "15           peace for     35\n",
       "16          war crimes     35\n",
       "17             lest we     34\n",
       "18           war peace     34\n",
       "19             the day     33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' ]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "N = 100\n",
    "\n",
    "# Bigrams\n",
    "text_bigrams = defaultdict(int)\n",
    "\n",
    "for text in df['all_hashtags_seg']:\n",
    "    for word in generate_ngrams(text, n_gram=2):\n",
    "        text_bigrams[word] += 1\n",
    "\n",
    "df_bigrams = pd.DataFrame(sorted(text_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_bigrams.columns = ['bigrams','count']\n",
    "df_bigrams.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
